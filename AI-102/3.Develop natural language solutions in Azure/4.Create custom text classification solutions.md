# Azure Language — Custom Text Classification

## What this module is about
Custom text classification lets you **assign labels (classes)** you define to text (documents).  
Example: classify a video game summary as **Action**, **Adventure**, **Strategy**, etc.

You learn to:
- Choose project type (single vs multi-label)
- Build a project (Language Studio or REST)
- Label/tag data, train, evaluate, improve
- Deploy and call the model from your app

---

## Types of classification projects

### Single-label classification
- **Exactly one** class per document.
- Example: A game summary can be **Action** OR **Strategy**, not both.
- Project kind: `customSingleLabelClassification`
- Runtime task kind: `CustomSingleLabelClassification`

### Multi-label classification
- **Multiple** classes per document.
- Example: A summary can be **Action** AND **Strategy**.
- Project kind: `customMultiLabelClassification`
- Runtime task kind: `CustomMultiLabelClassification`

---

## Model evaluation concepts

When predictions are wrong, you’ll often see:
- **False positive**: model predicts label X, but document isn’t actually X
- **False negative**: document is X, but model didn’t predict X

Azure Language reports:
- **Precision** = true positives / predicted positives
- **Recall** = true positives / actual positives
- **F1 score** = balance of precision and recall (single metric)

---

## Project life cycle (same in Language Studio and REST)
1. **Define labels** (your classes)
2. **Tag data** (assign class/classes to each document)
3. **Train model**
4. **View model results** (precision/recall/F1 per class)
5. **Improve model** (fix confusion, add better examples, rebalance)
6. **Deploy model** (make it callable via API)
7. **Classify text** (submit new docs to get predicted labels)

---

## Train/Test split options

### Automatic split (recommended with larger datasets)
- Put all docs into training during labeling
- Azure randomly splits, commonly **80% train / 20% test**

### Manual split (recommended with smaller datasets)
- You explicitly label docs as training vs testing
- Helps ensure class distribution is correct

---

## REST API pattern (asynchronous)

### Authentication header (required)
- `Ocp-Apim-Subscription-Key: <your-language-key>`

### Typical flow
- **POST** request starts a job → response is **202 Accepted**
- Grab `operation-location` (or `location`) header URL
- **GET** that URL until status is `succeeded`
- Read results from the response JSON

---

## Submit text for classification (runtime)
POST:
`<ENDPOINT>/language/analyze-text/jobs?api-version=<API-VERSION>`

Body includes:
- `analysisInput.documents[]` (id, language, text)
- `tasks[]` with:
  - `kind`: `CustomSingleLabelClassification` or `CustomMultiLabelClassification`
  - `parameters.projectName`
  - `parameters.deploymentName`

Then GET the `operation-location` URL to retrieve results.

---

# Module Assessment — Answers

## 1) Some books are both mystery and thriller. Which project type?
✅ **A multiple label classification project**

## 2) Training job is complete. Next step?
✅ **View your model details**  
(Then improve if needed, and deploy once performance is acceptable.)

## 3) How do you get results of the classification job?
✅ **Call the URL provided in the header of the request response**
