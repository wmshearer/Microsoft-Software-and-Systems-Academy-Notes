# Azure Speech

## What Azure Speech provides
Azure Speech enables **speech-enabled applications** with APIs for:
- **Speech to text** – Convert spoken audio into text
- **Text to speech** – Convert text into spoken audio
- **Speech translation** – Translate spoken input into other languages
- **Keyword recognition** – Detect trigger words or phrases
- **Intent recognition** – Infer semantic meaning from spoken input

This module focuses on **speech recognition** and **speech synthesis**.

---

## Azure Speech resource basics
To use Azure Speech, you must provision:
- A **dedicated Azure Speech resource**, or
- A **Microsoft Foundry resource**

You need the following to connect via SDK:
- **Location (region)** (example: `eastus`)
- **One subscription key**

Most SDK usage begins by creating a **SpeechConfig** object.

---

## Speech to Text (Speech Recognition)

### Core workflow
1. Create a **SpeechConfig** (key + region)
2. Create an **AudioConfig** (microphone or audio file)
3. Create a **SpeechRecognizer**
4. Call a recognition method (ex: `RecognizeOnceAsync`)
5. Process the **SpeechRecognitionResult**

### Result handling
- `RecognizedSpeech` → success
- `NoMatch` → audio parsed, but no speech detected
- `Canceled` → error occurred (check cancellation reason)

---

## Text to Speech (Speech Synthesis)

### Core workflow
1. Create **SpeechConfig**
2. Create **AudioConfig** (speaker, file, or null for stream)
3. Create **SpeechSynthesizer**
4. Call `SpeakTextAsync`
5. Process **SpeechSynthesisResult**

Successful synthesis:
- `Reason = SynthesizingAudioCompleted`
- `AudioData` contains synthesized audio

---

## Configure audio format and voices

### Audio format
You can control:
- File type
- Sample rate
- Bit depth

Configured on **SpeechConfig** using output format enumerations.

---

### Voices
- Voices are locale-based and named (ex: `en-GB-George`)
- Set on the **SpeechConfig** object
- Supports neural voices for natural speech

---

## Speech Synthesis Markup Language (SSML)

SSML gives advanced control over speech output:
- Speaking styles (cheerful, excited, etc.)
- Pauses and silence
- Pronunciation with phonemes
- Pitch, rate, and emphasis
- Dates, numbers, and abbreviations
- Multiple voices in one response
- Embedded audio

SSML is submitted using:
- `speak_ssml(...)` on **SpeechSynthesizer**

---

# Module Assessment — Answers

## 1) What information do you need from your Azure Speech service resource?
✅ **The location and one of the keys**

---

## 2) Which object specifies that speech input comes from an audio file?
✅ **AudioConfig**

---

## 3) How can you change the voice used in speech synthesis?
✅ **Set the SpeechSynthesisVoiceName property of the SpeechConfig object**
