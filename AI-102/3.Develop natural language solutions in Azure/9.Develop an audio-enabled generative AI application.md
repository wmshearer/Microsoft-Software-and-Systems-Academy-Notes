# Audio-Enabled Generative AI

## Overview
- Generative AI chat apps traditionally use **text-only** input.
- **Multimodal models** enable chat apps to process **text + audio** together.
- Audio-enabled generative AI supports:
  - Audio transcription
  - Audio understanding
  - Reasoning over spoken input
  - Voice-driven chat experiences

## Multimodal Models
- Multimodal models accept multiple input types:
  - Text
  - Audio
- Required for any prompt that includes audio.
- Available multimodal models in Microsoft Foundry include:
  - Microsoft Phi-4-multimodal-instruct
  - OpenAI gpt-4o
  - OpenAI gpt-4o-mini

## Deploy a Multimodal Model
- Audio prompts **will not work** with text-only models.
- You must deploy a **multimodal generative AI model** in Microsoft Foundry.
- Models are deployed from the Foundry model catalog.
- Once deployed, the model exposes an endpoint that supports audio content.

## Testing Audio-Based Prompts
- Microsoft Foundry Chat Playground supports:
  - Uploading audio files
  - Recording audio directly
  - Mixing audio and text in one prompt
- Useful for validating:
  - Transcription
  - Audio comprehension
  - Model reasoning over speech

## Develop an Audio-Based Chat App
- Development flow is similar to text-based chat apps:
  - Connect to deployed model endpoint
  - Submit prompt
  - Process response
- Key difference:
  - Prompts include **multi-part user messages**

## Multi-Part User Messages
- A single user message can contain:
  - Text content (instructions or questions)
  - Audio content (file or binary data)
- Conceptual structure:
  - `system` message (optional)
  - `user` message with multiple content items
- Example concept:
  `user message = text instruction + audio attachment`

## Audio Content Types
- Audio can be included as:
  - A URL to an audio file
  - Binary audio data
- Binary audio must be:
  - Base64 encoded
  - Embedded as a data URL
- Example concept:
  `data:audio/mp3;base64,<binary_audio_data>`

## APIs and SDKs
- Audio-based prompts can be submitted using:
  - Azure AI Model Inference API
  - OpenAI API
- SDKs abstract REST complexity:
  - Python SDK
  - .NET SDK

## Key Takeaways
- Audio input requires **multimodal models**
- Prompts must use **multi-part user messages**
- Audio can be passed as:
  - URL
  - Base64-encoded binary data
- App development mirrors text chat apps with added audio handling

## Module Assessment â€“ Answers
1. Which kind of model can you use to respond to audio input?
   - Multimodal models

2. How can you submit a prompt that asks a model to analyze an audio file?
   - Submit a prompt that contains a multi-part user message, containing both text content and audio content.

3. How can you include an audio in a message?
   - As a URL or as binary data
