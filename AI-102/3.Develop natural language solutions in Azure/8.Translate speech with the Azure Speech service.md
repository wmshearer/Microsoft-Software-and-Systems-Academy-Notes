# Azure Speech Translation

## Overview
Speech translation builds on **speech recognition** by:
- Recognizing and transcribing spoken input in a source language
- Translating the transcription into one or more target languages
- Optionally synthesizing the translated text back into speech

This enables **end-to-end, real-time speech-to-speech translation**.

---

## Azure resource requirements
To use Azure Speech Translation, you must provision:
- A **dedicated Azure Speech resource**, or
- A **multi-service Foundry Tools resource**

Required connection info:
- **Region/location** (example: `eastus`)
- **One subscription key**

These values are found under **Keys and Endpoint** in the Azure portal.

---

## Translate speech to text

### Core SDK pattern
Speech translation closely mirrors speech-to-text, with added translation configuration.

### Key objects
- **SpeechTranslationConfig**
  - Stores resource key and region
  - Defines **speech recognition language**
  - Defines **target translation language(s)**

- **AudioConfig**
  - Defines audio input source (microphone or audio file)

- **TranslationRecognizer**
  - Proxy client for the Azure Speech Translation API

### Execution flow
1. Create `SpeechTranslationConfig`
2. Set source language and target language(s)
3. Create `AudioConfig`
4. Create `TranslationRecognizer`
5. Call `RecognizeOnceAsync`
6. Process result

### Translation result details
- `Reason = RecognizedSpeech` → success
- `Text` → transcription in original language
- `Translations` → dictionary of translated text
  - Keys are ISO language codes (ex: `en`, `fr`)

---

## Synthesize translations (speech-to-speech)

You can convert translated text back into spoken audio in **two ways**:

### 1. Event-based synthesis (1:1 translation)
Best when:
- Translating from **one source language to one target language**

How it works:
- Set translated voice in `SpeechTranslationConfig`
- Handle the `Synthesizing` event on `TranslationRecognizer`
- Use `GetAudio()` to retrieve translated speech audio

---

### 2. Manual synthesis (1:many translation)
Best when:
- Translating into **multiple target languages**

How it works:
- Use `TranslationRecognizer` to get translated text
- Loop through the `Translations` dictionary
- Use a **SpeechSynthesizer** to synthesize each translation separately

This approach avoids event handling and supports multiple outputs.

---

# Module Assessment — Answers

## 1) Which SDK object specifies the target translation languages?
✅ **SpeechTranslationConfig**

---

## 2) Which SDK object is the proxy for the Translation API?
✅ **TranslationRecognizer**

---

## 3) When can you use the Synthesizing event to produce translated speech?
✅ **Only when translating to a single target language**
