# Vision-Enabled Generative AI

## Overview
- Generative AI chat apps traditionally use text input
- Multimodal models enable **vision-based interaction**
- Models can reason over **text + images** (and sometimes audio)
- Built using **Microsoft Foundry** with deployed multimodal models

## When to Use Vision-Enabled Generative AI
- Image understanding and description
- Visual question answering
- Image-based reasoning (charts, diagrams, photos)
- Combining user instructions with visual context

## Multimodal Models in Microsoft Foundry
To process image-based prompts, you must deploy a **multimodal model**.

### Common Multimodal Models
- Microsoft Phi-4-multimodal-instruct
- OpenAI gpt-4o
- OpenAI gpt-4o-mini

- These models support:
  - Text input
  - Image input
  - (Some also support audio)

## Testing Image-Based Prompts
- Use **Microsoft Foundry Chat Playground**
- Capabilities:
  - Upload local images
  - Add text instructions
  - Observe model responses
- Useful for:
  - Prompt testing
  - Rapid prototyping
  - Model behavior validation

## Vision-Based Chat Applications
Vision-based chat apps work similarly to text-based chat apps, with **one key difference**.

### Key Difference
- Prompts include **multi-part user messages**
- User message contains:
  - Text content
  - Image content

## Multi-Part Prompt Structure
- Messages array includes:
  - System message (instructions)
  - User message with multiple content items

### Prompt Structure (Conceptual)
- System:
  - Defines assistant behavior
- User:
  - Text instruction (what to do)
  - Image input (what to analyze)

## Image Content Types
Images can be included as:

### Image URL
- Public or accessible web URL
- Simple and lightweight

### Binary Image Data
- Local image file
- Must be:
  - Base64 encoded
  - Wrapped in a data URL format

## Image Submission Format
- Image content uses `image_url`
- Binary images use:
  - `data:image/jpeg;base64,<binary_image_data>`

## APIs and SDKs
Vision-enabled prompts can be submitted using:
- Azure AI Model Inference API
- OpenAI API
- Language-specific SDKs:
  - Python
  - .NET

SDKs abstract REST complexity and handle:
- Message formatting
- Authentication
- Response parsing

## End-to-End Vision Chat Flow
1. Deploy a multimodal model
2. Connect to model endpoint
3. Build multi-part prompt (text + image)
4. Submit prompt via API or SDK
5. Process model response

## Key Exam Concepts
- Vision input requires **multimodal models**
- Images are included inside **user messages**
- Prompts must be **multi-part**, not separate messages
- Images can be URLs or binary data
- Same chat mechanics as text-only apps, plus image content

## Module Assessment â€“ Answers
1. Which model responds to visual input?
   - Multimodal models

2. How do you ask a model to analyze an image?
   - Submit a prompt with a multi-part user message containing both text and image content

3. How can you include an image in a message?
   - As a URL or as binary data
