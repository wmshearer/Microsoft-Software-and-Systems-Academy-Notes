# Prompt flow and building LLM applications in Microsoft Foundry

## Why prompt flow
The power of Large Language Models (LLMs) comes from applying them to real problems by combining models with data sources and application logic. Prompt flow is used to develop, test, tune, and deploy LLM applications, and is available in both the Microsoft Foundry portal and Azure Machine Learning studio. This module focuses on prompt flow in Microsoft Foundry, but the concepts apply to both environments.

## Prompts and flows
A prompt is the input given to an LLM application, such as a question, instruction, or set of guidelines that drive the modelâ€™s output. Prompt effectiveness depends on how clearly it conveys intent and desired outcome.

A flow represents the sequence of actions used to accomplish a task with an LLM. It encapsulates the entire pipeline from receiving input, interacting with the model, and producing output or performing an action.

## LLM application development lifecycle
The lifecycle of an LLM application consists of four iterative stages:

### Initialization
Define the use case and design the solution by:
- Defining the objective
- Collecting a representative sample dataset
- Building a basic prompt
- Designing the flow
Sample datasets should be diverse, cover edge cases, and exclude sensitive data.

### Experimentation
Develop and run the flow against a small dataset to test behavior. Evaluate prompt performance and iteratively modify prompts or flow structure until results are satisfactory.

### Evaluation and refinement
Assess the flow using a larger dataset to validate generalization and identify bottlenecks or areas for optimization. Changes should first be tested on smaller datasets before re-evaluating at scale.

### Production
Optimize the flow for efficiency, deploy it to an endpoint, and monitor performance using usage data and end-user feedback. If issues arise, revert to experimentation and refine.

## Core components of prompt flow

### Flow structure
A flow consists of:
- Inputs: data passed into the flow (strings, numbers, booleans, etc.)
- Nodes: tools that process data or perform tasks
- Outputs: results produced by the flow
Nodes can consume flow inputs or outputs from other nodes, forming a pipeline.

### Tools in prompt flow
Common built-in tools include:
- LLM tool: creates prompts and interacts with language models
- Python tool: runs custom Python logic
- Prompt tool: prepares prompts as strings for complex scenarios
Tools are reusable executable units, and multiple tools can be combined in a single flow. Custom tools can be created when built-in tools are insufficient.

### Flow types
- Standard flow: general-purpose LLM application development
- Chat flow: optimized for conversational scenarios
- Evaluation flow: focused on evaluating and improving model or application performance

## Connections and runtimes

### Connections
Connections authorize flows to securely access external services, APIs, or data sources. Connection details (endpoints, keys, credentials) are stored securely in Azure Key Vault and reused across flows.

Built-in tools that require connections include:
- Azure OpenAI: LLM or Python tools
- OpenAI: LLM or Python tools
- Azure AI Search: Vector DB Lookup or Python tools
- Serp: Serp API or Python tools
- Custom services: Python tools

Connections simplify credential management and enable secure data transfer.

### Runtimes
Runtimes provide the compute needed to execute flows. A runtime combines:
- Compute resources
- An environment defining required packages and libraries
A default environment is available for quick testing; custom environments can be created when additional dependencies are required.

## Optimizing, deploying, and monitoring flows

### Variants
Variants are alternative configurations of a tool node (currently supported for LLM tools). Variants allow comparison of different prompt contents or settings to improve quality, speed, and productivity, and enable side-by-side evaluation of results.

### Deploying flows
Flows can be deployed to online endpoints. An endpoint provides a URL and key that external applications can call to trigger the flow and receive real-time responses, enabling chat or agentic integrations.

### Monitoring and evaluation metrics
Monitoring evaluates how well an LLM application performs in real-world usage. Feedback and comparisons against ground truth are used to assess quality.

Key evaluation metrics include:
- Groundedness: alignment with source data
- Relevance: pertinence of output to input
- Coherence: logical flow and readability
- Fluency: grammatical and linguistic correctness
- Similarity: semantic match to ground truth
When performance degrades, revert to experimentation and iteratively improve the flow.

# Module assessment answers

## Q1: A flow uses an LLM tool to generate text with a GPT-3.5 model. What do you need to create to ensure prompt flow can securely call the deployed model from Azure OpenAI?
Answer: Connections

## Q2: You want to integrate your flow with an online website. What do you need to do to easily integrate your flow?
Answer: Deploy your flow to an endpoint

## Q3: After deployment, you notice that your flow is underperforming. Which stage in the development lifecycle should you revert back to?
Answer: Experimentation