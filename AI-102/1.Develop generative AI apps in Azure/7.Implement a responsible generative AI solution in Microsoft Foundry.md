# Responsible Generative AI (Microsoft Guidance)

## Overview
- Generative AI can produce highly realistic content
- This power introduces risks that must be proactively managed
- Microsoft guidance extends the Responsible AI Standard for generative models

## Responsible Generative AI Lifecycle (4 Stages)
- Map potential harms
- Measure potential harms
- Mitigate harms using layered controls
- Manage the solution responsibly post-deployment

> Closely aligned with the NIST AI Risk Management Framework

## 1. Map Potential Harms
### Purpose
- Identify and understand risks specific to:
  - Model choice
  - Fine-tuning
  - Grounding data
  - Intended and unintended use

### Common Harm Types
- Offensive, discriminatory, or hateful content
- Hallucinations / factual inaccuracies
- Illegal or unethical guidance

### Key Actions
- Review service transparency notes and model documentation
- Use the Microsoft Responsible AI Impact Assessment template
- Identify, prioritize, test, and document harms

### Prioritization Criteria
- Likelihood of occurrence
- Severity of impact
- Intended use vs potential misuse

### Red Team Testing
- Deliberately probe for harmful outputs
- Document successful exploit attempts
- Extend traditional security red teaming to AI behavior

## 2. Measure Potential Harms
### Goal
- Establish a baseline for harmful outputs
- Track improvement as mitigations are applied

### Measurement Process
- Prepare prompts likely to trigger harms
- Generate outputs
- Categorize output using predefined harm criteria

### Testing Approaches
- Start with manual testing
- Scale using automated evaluation (classification models)
- Periodically revalidate with manual testing

## 3. Mitigate Potential Harms (Layered Defense)
### Model Layer
- Choose the least powerful model that meets requirements
- Fine-tune models to narrow scope and reduce risk

### Safety System Layer
- Platform protections (Microsoft Foundry):
  - Content filters (hate, sexual, violence, self-harm)
  - Severity levels: safe, low, medium, high
  - Abuse detection and alerting

### System Message & Grounding Layer
- Strong system prompts to enforce behavior
- Prompt engineering to constrain outputs
- Use RAG with trusted data sources

### User Experience Layer
- Constrain inputs and outputs via UI design
- Validate user inputs and model responses
- Be transparent about limitations and risks

## 4. Manage the Solution Responsibly
### Pre-release Reviews
- Legal
- Privacy
- Security
- Accessibility

### Release & Operations Best Practices
- Phased rollout (limited users first)
- Incident response and rollback plans
- Ability to block:
  - Harmful outputs
  - Users
  - Applications
  - Client IPs
- User feedback and content reporting
- Telemetry for quality, safety, and usability (privacy-compliant)

## Microsoft Foundry Content Safety
### Purpose
- Additional protections beyond base content filters

### Key Features
- Prompt shields (input attack detection)
- Groundedness detection
- Protected material detection (copyright)
- Custom harm categories

## Module Assessment â€“ Key Takeaways
- AI Impact Assessments document purpose, usage, and risks
- Content filters mitigate harm at the Safety System layer
- Phased delivery enables safer validation before broad release
