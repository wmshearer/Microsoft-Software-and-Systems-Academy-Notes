# Evaluating generative AI applications in Microsoft Foundry

## Why evaluation matters
Evaluating generative AI applications is essential for quality assurance, user satisfaction, and continuous improvement. Evaluation helps identify inaccurate or irrelevant responses, ensures the application meets user expectations, and supports iterative refinement so the app remains effective and valuable over time.

## What can be evaluated
Evaluation can be applied at different levels:
- Individual language models: assess how a single model responds to inputs.
- Chat flows: evaluate an orchestrated flow that may combine multiple models and Python logic.
Evaluation can start with individual models and expand to full chat flows as the solution matures.

## Model interaction context
A language model evaluation considers:
- Input prompt
- Generated output
- (Optional) comparison against expected output (ground truth)

A chat flow evaluation considers:
- Flow input
- Execution across multiple nodes
- Final output
Both the complete flow and individual components can be evaluated.

## Evaluation approaches

### Model benchmarks
Model benchmarks are publicly available metrics used to compare models across datasets. They help during model selection before deployment.

Benchmarks described include:
- Accuracy: exact match between generated text and correct answer
- Coherence: smoothness, readability, and human-like flow
- Fluency: grammatical correctness and appropriate vocabulary usage
- GPT similarity: semantic similarity between generated output and ground truth
Model benchmarks are available in the Microsoft Foundry portal.

### Manual evaluations
Manual evaluations use human judgement to assess response quality. Humans can evaluate aspects that automated metrics may miss, such as contextual relevance, usefulness, and user satisfaction.

Manual evaluation is useful:
- Early in development for rapid experimentation
- In production to assess real-world quality

### AI-assisted metrics
AI-assisted metrics use AI models to evaluate responses for:
- Generation quality (creativity, coherence, style adherence)
- Risk and safety (harmful, biased, or unsafe content)

### NLP-based metrics
NLP metrics quantify overlap between generated output and expected response (ground truth). These are useful for classification, retrieval, and translation tasks.

Metrics listed:
- F1 score: ratio of shared words between generated and ground truth text
- BLEU
- METEOR
- ROUGE

## Manual evaluation workflow in Microsoft Foundry

### Prepare test prompts
Create a diverse set of prompts representing real usage, edge cases, and potential failure scenarios. This ensures comprehensive evaluation.

### Test models in the chat playground
The chat playground allows testing a deployed language model directly. You can:
- Enter prompts
- Inspect responses
- Modify prompts or system messages
- Retest to observe improvements
This is ideal for early-stage development.

### Evaluate multiple prompts
Manual evaluations allow batch testing using a dataset of prompts (and optional expected responses). Responses can be rated using thumbs up/down, helping guide improvements to prompts, system messages, model choice, or parameters.

## Automated evaluations

### Evaluation data
Automated evaluations require a dataset of prompts and responses, optionally with ground truth. Evaluation datasets can be:
- Manually created
- Extracted from an existing application
- Generated by an AI model and then refined

### Automated evaluators
Automated evaluators assess:
- AI Quality: coherence, relevance, and NLP metrics such as F1, BLEU, METEOR, and ROUGE
- Risk and safety: detection of violence, hate, sexual content, and self-harm related content
Automated evaluations support scalable, objective performance assessment.

# Module assessment answers

## Q1: Which evaluation technique can you use to apply your own judgement about the quality of responses to a set of specific prompts?
Answer: Manual evaluations

## Q2: Which evaluator compares generated responses to ground truth based on standard metrics?
Answer: F1 Score

## Q3: Which evaluator metric uses an AI model to judge the structure and logical flow of ideas in a response?
Answer: Coherence
